{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14813f5-3cb9-484c-a7b4-75896d4bec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c68cbcf2-4631-4c8e-b0f8-1a3491edd5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NLLB.en-gn/NLLB.en-gn.en', 'r') as f:\n",
    "    ENs = f.read().splitlines()\n",
    "with open('NLLB.en-gn/NLLB.en-gn.gn', 'r') as f:\n",
    "    GNs = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eb22d7-5db8-4688-99ef-9e59ec2e2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENs = ENs[:3000]\n",
    "GNs = GNs[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e7ad090-e092-4079-b07a-3e2f8779b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Guarani language code\n",
    "new_lang_code = \"gn_XX\"\n",
    "\n",
    "# Prepare data entries with language tokens\n",
    "data = []\n",
    "for en, gn in zip(ENs, GNs):\n",
    "    data.append({\n",
    "        \"src\": f\"en_XX {en}\",\n",
    "        \"tgt\": f\"{new_lang_code} {gn}\"\n",
    "    })\n",
    "    \n",
    "\n",
    "raw_dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae4ef360-e34c-4bf3-a91a-63a1dce4f377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MBartScaledWordEmbedding(250055, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\")\n",
    "\n",
    "# Add Guarani token and resize embeddings\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [new_lang_code]})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ceb113-f812-4ad0-b6f6-ddac44792744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fec729705d421b8783f3a71f5c376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def preprocess(example):\n",
    "    \n",
    "    inputs = tokenizer(example[\"src\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    labels = tokenizer(example[\"tgt\"], max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        \n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# remove_columns=raw_dataset.column_names\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c641a98a-6e21-4265-a5ed-082dc95fd3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250055, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250055, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250055, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250055, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mps.set_per_process_memory_fraction(0.0)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ff1a3a-2667-48fc-908d-a48126fe4139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4z/_xsgjh514rn3gfx85sk5v_w00000gn/T/ipykernel_7706/2273625128.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/500 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data collector\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(new_lang_code)\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart50-finetuned-gn\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    predict_with_generate=True,\n",
    "    #generation_max_length=64,\n",
    "    #generation_num_beams=4,\n",
    "    #report_to=\"none\",\n",
    "    max_steps=500,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a88810-2f90-44e7-bde8-179d3747a85d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
