{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517fee5c-921c-443b-83e0-c7356b6cc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "chrf = evaluate.load(\"chrf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acff742-13d3-4bc1-a5f1-fb2621a5ad54",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open('Flores/eng_Latn.dev','r') # string of english sentences \n",
    "file2=open('Flores/grn_Latn.dev','r') # string of guarani counterparts\n",
    "\n",
    "\n",
    "ENG=file1.read()\n",
    "GRN=file2.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c37c6e-4ebe-4c23-9b5c-f719566157a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGs=ENG.splitlines() #sentence by sentence list of the english data\n",
    "GRNs=GRN.splitlines() #sentence by sentence list of the guarani data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267d551f-aa74-43a1-96b2-9d13f5b6e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "model_dir = \"mbart50-finetuned-gn/checkpoint-1248\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_dir)\n",
    "\n",
    "# Load the model\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5fdea35-0ce8-4d86-8db7-94ddd93fe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26dbf31a-b941-45c2-8b33-71bf51cad596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "model_dir = \"mbart50-finetuned-gn/checkpoint-625\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_dir)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "# Add missing lang code if needed\n",
    "if \"gn_XX\" not in tokenizer.lang_code_to_id:\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [\"gn_XX\"]})\n",
    "    tokenizer.lang_code_to_id[\"gn_XX\"] = len(tokenizer) - 1\n",
    "    model.resize_token_embeddings(len(tokenizer), mean_resizing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ccde38a-3c73-4b80-85eb-af604ff08296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Translation test\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "target_lang = \"gn_XX\"\n",
    "input_text = \"Hello\"\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang])\n",
    "\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bce3b988-1527-435d-8cd5-597b8f23956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation(source):\n",
    "    # ensure tokenizer and target are correct\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    target_lang = \"gn_XX\"\n",
    "\n",
    "    translation_list=[]\n",
    "    for sent in source:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[target_lang])\n",
    "        translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "        translation_list.append(translated_text)\n",
    "    return translation_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48462cd8-13ca-4460-a6c0-5c50e7346269",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=translation(ENGs)\n",
    "references=[[sent] for sent in GRNs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b6aee96-51ac-499a-a93c-fbfc81b0fe19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.0, 'char_order': 6, 'word_order': 2, 'beta': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "results = chrf.compute(predictions=predictions, references=references, word_order=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b93f3-2439-4a22-8763-8ace59a5ee99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
